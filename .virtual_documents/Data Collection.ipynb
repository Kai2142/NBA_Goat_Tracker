


import pandas as pd
from bs4 import BeautifulSoup
import requests 
import time
import re


headers = {"User Agent": "Mozilla/5.0"}


# Read player_extensions csv
url_extensions_df = pd.read_csv("data/player_extensions.csv")
# Convert Extension Column to list
url_extensions = url_extensions_df["Player Extension"].tolist()


# Create List with player links
player_links = []
def concat_urls(url_extension):
    for ext in url_extension:
        base_url = "https://www.basketball-reference.com/"
        player_url = ext
        full_url = base_url + player_url
        player_links.append(full_url)
concat_urls(url_extensions)


print(player_links[0])





def scrape_ppg(full_url):
    url = full_url
    try:
        time.sleep(7)  
        r = requests.get(url, headers=headers)
        r.raise_for_status()  # Raise an exception for bad HTTP status codes
        soup = BeautifulSoup(r.content, "html.parser")

        p1_div = soup.find("div", {"class": "p1"})
        if p1_div:
            ppg_value_element = p1_div.find_all("p")[3] 
            ppg_value = ppg_value_element.get_text()
            return ppg_value.strip()
        else:
            return "PPG information not found"
    except requests.RequestException as e:
        return f"An error occurred: {e}"

# print(scrape_ppg(player_links[0]))





def scrape_trb(full_url):
    url = full_url
    try:
        time.sleep(7)  
        r = requests.get(url, headers=headers)
        r.raise_for_status()  
        soup = BeautifulSoup(r.content, "html.parser")

        p1_div = soup.find("div", {"class": "p1"})
        if p1_div:
            ppg_value_element = p1_div.find_all("p")[5] 
            ppg_value = ppg_value_element.get_text()
            return ppg_value.strip()
        else:
            return "PPG information not found"
    except requests.RequestException as e:
        return f"An error occurred: {e}"

print(scrape_trb(player_links[0]))





def scrape_ast(full_url):
    url = full_url
    try:
        time.sleep(7)  
        r = requests.get(url, headers=headers)
        r.raise_for_status()  
        soup = BeautifulSoup(r.content, "html.parser")

        p1_div = soup.find("div", {"class": "p1"})
        if p1_div:
            ppg_value_element = p1_div.find_all("p")[7] 
            ppg_value = ppg_value_element.get_text()
            return ppg_value.strip()
        else:
            return "PPG information not found"
    except requests.RequestException as e:
        return f"An error occurred: {e}"

print(scrape_ast(player_links[0]))





def scrape_mvp_count(full_url):
    url = full_url
    try:
        time.sleep(7)  
        r = requests.get(url, headers=headers)
        r.raise_for_status()  
        soup = BeautifulSoup(r.content, "html.parser")

        # Find the <ul> element with id "bling"
        bling_ul = soup.find('ul', {'id': 'bling'})
        
        if bling_ul:
            mvp_count = None
            for li in bling_ul.find_all('li'):
                a_tag = li.find('a')
                if a_tag and "MVP" in a_tag.text:
                    if "MVP" in a_tag.text and "Finals" not in a_tag.text:
                        mvp_count = re.search(r'\d+x', a_tag.text)
                        if mvp_count:
                            return mvp_count.group().replace('x', '')
            
            if mvp_count is None:
                return "MVP count not found"
        else:
            return "Bling section not found"
    except requests.RequestException as e:
        return f"An error occurred: {e}"

# print(scrape_mvp_count(player_links[0]))





def scrape_fmvp_count(full_url):
    url = full_url
    try:
        time.sleep(7)  
        r = requests.get(url, headers=headers)
        r.raise_for_status()  
        soup = BeautifulSoup(r.content, "html.parser")

        # Find the <ul> element with id "bling"
        bling_ul = soup.find('ul', {'id': 'bling'})

        if bling_ul:
            fmvp_count = None
            for li in bling_ul.find_all('li'):
                a_tag = li.find('a')
                if a_tag and "Finals MVP" in a_tag.text:
                    fmvp_count = re.search(r'\d+x', a_tag.text)
                    if fmvp_count:
                        return fmvp_count.group().replace('x', '')
            
            if fmvp_count is None:
                return "Finals MVP count not found"
        else:
            return "Bling section not found"
    except requests.RequestException as e:
        return f"An error occurred: {e}"

# Call the function with the player link
# print(scrape_fmvp_count(player_links[0]))





def scrape_anbat_count(full_url):
    url = full_url
    try:
        time.sleep(7)
        r = requests.get(url, headers=headers)
        r.raise_for_status()
        soup = BeautifulSoup(r.content, "html.parser")

        # Find the <ul> element with id "bling"
        bling_ul = soup.find('ul', {'id': 'bling'})

        if bling_ul:
            anbat_count = None
            for li in bling_ul.find_all('li'):
                a_tag = li.find('a')
                if a_tag and "All-NBA" in a_tag.text:
                    anbat_count = re.search(r'\d+x', a_tag.text)
                    if anbat_count:
                        return anbat_count.group().replace('x', '')

            if anbat_count is None:
                return "All-NBA Teams count not found"
        else:
            return "Bling section not found"
    except requests.RequestException as e:
        return f"An error occurred: {e}"

# Call the function with the player link
# print(scrape_anbat_count(player_links[0]))





def scrape_nba_champ_count(full_url):
    url = full_url
    try:
        time.sleep(7)
        r = requests.get(url, headers=headers)
        r.raise_for_status()
        soup = BeautifulSoup(r.content, "html.parser")

        # Find the <ul> element with id "bling"
        bling_ul = soup.find('ul', {'id': 'bling'})

        if bling_ul:
            nba_champ_count = None
            for li in bling_ul.find_all('li'):
                a_tag = li.find('a')
                if a_tag and "NBA Champ" in a_tag.text:
                    nba_champ_count = re.search(r'\d+x', a_tag.text)
                    if nba_champ_count:
                        return nba_champ_count.group().replace('x', '')

            if nba_champ_count is None:
                return "NBA Championships count not found"
        else:
            return "Bling section not found"
    except requests.RequestException as e:
        return f"An error occurred: {e}"
    
# Call the function with the player link
print(scrape_nba_champ_count(player_links[0]))





def scrape_all_def_team_count(full_url):
    url = full_url
    try:
        time.sleep(7)
        r = requests.get(url, headers=headers)
        r.raise_for_status()
        soup = BeautifulSoup(r.content, "html.parser")
                # Find the <ul> element with id "bling"
        bling_ul = soup.find('ul', {'id': 'bling'})

        if bling_ul:
            all_def_team_count = None
            for li in bling_ul.find_all('li'):
                a_tag = li.find('a')
                if a_tag and "All-Defensive" in a_tag.text:
                    all_def_team_count = re.search(r'\d+x', a_tag.text)
                    if all_def_team_count:
                        return all_def_team_count.group().replace('x', '')

            if all_def_team_count is None:
                return "NBA Championships count not found"
        else:
            return "Bling section not found"
    except requests.RequestException as e:
        return f"An error occurred: {e}"
        
print(scrape_all_def_team_count(player_links[0]))





def scrape_per_count(full_url):
    url = full_url
    try:
        time.sleep(7)  
        r = requests.get(url, headers=headers)
        r.raise_for_status()  
        soup = BeautifulSoup(r.content, "html.parser")

        p1_div = soup.find("div", {"class": "p3"})
        if p1_div:
            ppg_value_element = p1_div.find_all("p")[1] 
            ppg_value = ppg_value_element.get_text()
            return ppg_value.strip()
        else:
            return "PPG information not found"
    except requests.RequestException as e:
        return f"An error occurred: {e}"

print(scrape_per_count(player_links[0]))











def scrape_ppg_list(player_link):
    ppg_list = []
    for player in player_link:
        ppg_list.append(scrape_ppg(player))
    return ppg_list
ppg_values = scrape_ppg_list(player_links)
print(ppg_values)


def scrape_trb_list(player_link):
    trb_list = []
    for player in player_link:
        trb_list.append(scrape_trb(player))
    return trb_list
trb_values = scrape_trb_list(player_links)
print(trb_values)


def scrape_ast_list(player_link):
    ast_list = []
    for player in player_link:
        ast_list.append(scrape_trb(player))
    return ast_list
ast_values = scrape_ast_list(player_links)
print(ast_values)


def scrape_mvp_list(player_link):
    mvp_list = []
    for player in player_link:
        mvp_list.append(scrape_mvp_count(player))
    return mvp_list
mvp_values = scrape_mvp_list(player_links)
print(mvp_values)
        


def scrape_anbat(player_link):
    anbat_list = []
    for player in player_links:
        anbat_list.append(scrape_anbat_count(player))
    return anbat_list
anbat_values = scrape_anbat(player_links)
print(anbat_values)
        


def scrape_all_def_team(player_link):
    all_def_team_list = []
    for player in playerlist:
        all_def_team_.list.append(scrape_all_def_team(player))
    return all_def_team_list
all_def_team_values = scrape_all_def_team(player_links)
print(all_def_team_values)


def scrape_per(player_link)
    per_list = []
    for player in player_list:
        per_list.list.append(scrape_per_count(player))
    return per_list
per_values = scrape_per(player_links)
print(per_values)


df = url_extensions_df['Player Name'] 
# df = ['PPG'] = ppg_values
ppg_series = pd.DataFrame(ppg_values, columns=['PPG'])
# df = url_extensions_df[['PPG']] = df_1
df = pd.concat([df, ppg_series], axis=1)
df
